# Awesome-Force-Tactile-VLA

With significant advances in **Vision-Language-Action (VLA)** models based on large-scale imitation learning, integrating physics-related modalities such as force, tactile into VLA has emerged as a promising paradigm. This approach harnesses fine-grained interaction data between robots and objects, while providing intuitive and powerful guidance for policies to produce precise behaviors-especially in contact-rich manipulation tasks such as insertion.

This repository summarizes recent advances in the **VLA + Force/Tactile** paradigm and provides a classification of relevant works.

**Contributions are welcome! Please feel free to submit an issue or reach out via email to add papers!**

If you find this repository useful, please giving this list a star ⭐. Feel free to share it with others!

# **Force Sensing & Force-Aware Manipulation**

| Method | Title | Venue | Date | Code/Project | Key feature/finding |
| --- | --- | --- | --- | --- | --- |
| [FoAR](http://arxiv.org/abs/2411.15753) | FoAR: Force-Aware Reactive Policy for Contact-Rich Robotic Manipulation | Arxiv | 5/2025 | [Project](https://tonyfang.net/FoAR/) | <details><summary>Details</summary>High-frequency F/T + vision fusion via future contact predictor; dynamic force usage adjustment; >baseline in occlusion/disturbance tasks</details> |
| [ACP](http://arxiv.org/abs/2410.09309) | Adaptive Compliance Policy: Learning Approximate Compliance for Diffusion Guided Control | Arxiv | 3/2025 | [Project](https://adaptive-compliance.github.io/) | <details><summary>Details</summary>Spatially/temporally adaptive compliance from demos; >50% improvement over visuomotor policies in contact-rich tasks</details> |
| [FILIC](http://arxiv.org/abs/2509.17053) | FILIC: Dual-Loop Force-Guided Imitation Learning with Impedance Torque Control for Contact-Rich Manipulation Tasks | Arxiv | 9/2025 | [Github](https://github.com/TATP-233/FILIC) | <details><summary>Details</summary>Transformer IL + impedance control; joint torque-based force estimation; outperforms vision-only by safer/compliant manipulation</details> |
| [Cai2025](http://arxiv.org/abs/2509.19261) | Imitation-Guided Bimanual Planning for Stable Manipulation under Changing External Forces | Arxiv | 9/2025 | [Video](https://youtu.be/3DhbUsv4eDo) | <details><summary>Details</summary>Stable uni/bi-manual grasp transitions; hierarchical IL + QP planning; improved efficiency under external forces</details> |
| [FACTR](https://jasonjzliu.com/factr/) | FACTR: Force-Attending Curriculum Training for Contact-Rich Policy Learning | Tech Report | 2025 | [Project](https://jasonjzliu.com/factr/) | <details><summary>Details</summary>Bilateral teleop + curriculum training; 43% generalization improvement using force attention in transformer policy</details> |
| [RH20T](https://rh20t.github.io) | RH20T: A Comprehensive Robotic Dataset for Learning Diverse Skills in One-Shot | Tech Report | 2025 | [Project](https://rh20t.github.io) | <details><summary>Details</summary>110k+ multi-modal (vis/force/audio/action) sequences; 100s real-world skills with language descriptions</details> |
| [FORGE](http://arxiv.org/abs/2408.04587) | FORGE: Force-Guided Exploration for Robust Contact-Rich Manipulation under Uncertainty | Arxiv | 1/2025 | [Project](https://noseworm.github.io/forge/) | <details><summary>Details</summary>Force threshold + dynamics randomization; adaptive success prediction; snap-fit + gear assembly</details> |
| [Chen2025](http://arxiv.org/abs/2501.10356) | DexForce: Extracting Force-informed Actions from Kinesthetic Demonstrations for Dexterous Manipulation | Arxiv | 3/2025 |  | <details><summary>Details</summary>Force-informed actions from kinesthetic demos; 76% success vs 0% baseline; essential for precision tasks</details> |
| [Zhi2025](http://arxiv.org/abs/2505.20829) | Learning a Unified Policy for Position and Force Control in Legged Loco-Manipulation | Arxiv | 10/2025 |  | <details><summary>Details</summary>Force estimation from states; 39.5% success boost; quadruped/humanoid versatility</details> |
| [Lee2025b](http://arxiv.org/abs/2509.19047) | ManipForce: Force-Guided Policy Learning with Frequency-Aware Representation for Contact-Rich Manipulation | Arxiv | 9/2025 |  | <details><summary>Details</summary>Handheld F/T+RGB capture; FMT cross-attention; 83% success across 6 tasks vs RGB-only baseline</details> |
| [Xu2025](http://arxiv.org/abs/2506.16685) | Compliant Residual DAgger: Improving Real-World Contact-Rich Manipulation with Human Corrections | Arxiv | 7/2025 | [Project](https://compliant-residual-dagger.github.io/) | <details><summary>Details</summary>Compliant intervention interface; >50% success boost with minimal corrections; book flipping/belt assembly</details> |
| [Brouwer2025](http://arxiv.org/abs/2508.19476) | Gentle Object Retraction in Dense Clutter Using Multimodal Force Sensing and Imitation Learning | Arxiv | 8/2025 |  | <details><summary>Details</summary>Triaxial tactile + wrench estimation; 80% improvement over vision-only; constrained clutter extraction</details> |
| [ForceVLA](http://arxiv.org/abs/2505.22159) | ForceVLA: Enhancing VLA Models with a Force-aware MoE for Contact-rich Manipulation | Arxiv | 9/2025 | [Project](https://sites.google.com/view/forcevla2025) | <details><summary>Details</summary>FVLMoE fusion module; 23.2% success boost; 80% plug insertion success</details> |
| [TA-VLA](http://arxiv.org/abs/2509.07962) | TA-VLA: Elucidating the Design Space of Torque-aware Vision-Language-Action Models | Arxiv | 9/2025 |  | <details><summary>Details</summary>Torque adapters in decoder + auxiliary prediction; improves contact-rich benchmarks</details> |
| [MOMA-Force](http://arxiv.org/abs/2308.03624) | MOMA-Force: Visual-Force Imitation for Real-World Mobile Manipulation | Arxiv | 8/2023 | [Project](https://visual-force-imitation.github.io) | <details><summary>Details</summary>Visual-force imitation; smaller contact forces/variances; high success in household tasks</details> |
| [DOGlove](https://do-glove.github.io/) | DOGlove: Dexterous Manipulation with a Low-Cost Open-Source Haptic Force Feedback Glove | Tech Report | 2025 | [Project](https://do-glove.github.io/) | <details><summary>Details</summary>$600 haptic+force glove; 21DoF motion + 5DoF feedback; blind teleoperation success</details> |

---

# **Tactile Sensing & Visuo-Tactile Manipulation**

| Method | Title | Venue | Date | Code/Project | Key feature/finding |
| --- | --- | --- | --- | --- | --- |
| [Surformer v2](http://arxiv.org/abs/2509.04658) | Surformer v2: A Multimodal Classifier for Surface Understanding from Touch and Vision | Arxiv | 9/2025 |  | <details><summary>Details</summary>Late(decision-level) fusion with Efficient V-Net (vision) + encoder-only transformer (tactile); 99%+ accuracy on Touch and Go dataset; real-time inference</details> |
| [Tac2Motion](http://arxiv.org/abs/2509.17812) | Tac2Motion: Contact-Aware Reinforcement Learning with Tactile Feedback for Robotic Hand Manipulation | Arxiv | 9/2025 | [Video](https://youtu.be/poeJBPR7urQ) | <details><summary>Details</summary>Tactile reward shaping + embedding; firm grasping + smooth finger gaiting; real-world transfer to Shadow Robot for lid opening</details> |
| [CLTP](http://arxiv.org/abs/2505.08194) | CLTP: Contrastive Language-Tactile Pre-training for 3D Contact Geometry Understanding | Arxiv | 5/2025 | [Project](https://sites.google.com/view/cltp/) | <details><summary>Details</summary>50k+ tactile 3D point cloud-language pairs; contact-state-aware (location/shape/force); zero-shot 3D classification + LLM interaction</details> |
| [TactileAloha](https://doi.org/10.1109/LRA.2025.3585396) | TactileAloha: Learning Bimanual Manipulation With Tactile Sensing | IEEE RA-L | 8/2025 |  | <details><summary>Details</summary>Aloha system + tactile sensors; transformer policy with action chunking; 11% improvement in texture tasks (zip tie/Velcro)</details> |
| [VTLA](http://arxiv.org/abs/2505.09577) | VTLA: Vision-Tactile-Language-Action Model with Preference Learning for Insertion Manipulation | Arxiv | 5/2025 | [Project](https://sites.google.com/view/vtla) | <details><summary>Details</summary>DPO for continuous tasks; >90% success on unseen peg shapes; Sim2Real for peg-in-hole</details> |
| [Wu2025a](http://arxiv.org/abs/2409.17549) | Canonical Representation and Force-Based Pretraining of 3D Tactile for Dexterous Visuo-Tactile Policy Learning | Arxiv | 6/2025 | [Project](https://3dtacdex.github.io) | <details><summary>Details</summary>Canonical 3D tactile + force pretraining; 78% success across 4 dexterous tasks using spatial/force info</details> |
| [ConViTac](http://arxiv.org/abs/2506.20757) | ConViTac: Aligning Visual-Tactile Fusion with Contrastive Representations | Arxiv | 6/2025 |  | <details><summary>Details</summary>Contrastive Embedding Conditioning (CEC); 12% accuracy boost in material classification/grasping prediction</details> |
| [VLA-Touch](http://arxiv.org/abs/2507.17294) | VLA-Touch: Enhancing Vision-Language-Action Models with Dual-Level Tactile Feedback | Arxiv | 7/2025 | [Github](https://github.com/jxbi1010/VLA-Touch) | <details><summary>Details</summary>No fine-tuning VLA; tactile-language model + diffusion controller; improves planning/execution precision</details> |
| [FORTE](http://arxiv.org/abs/2506.18960) | FORTE: Tactile Force and Slip Sensing on Compliant Fingers for Delicate Manipulation | Arxiv | 6/2025 | [Project](https://merge-lab.github.io/FORTE) | <details><summary>Details</summary>3D-printed fin-ray grippers + air channels; 0.2N force accuracy, 100ms slip detection; 98.6% fragile object success</details> |
| [ViTaL](http://arxiv.org/abs/2506.13762) | Touch begins where vision ends: Generalizable policies for contact-rich manipulation | Arxiv | 6/2025 | [Project](https://vitalprecise.github.io) | <details><summary>Details</summary>VLM reaching + reusable tactile policy; ~90% success in unseen environments; residual RL + tactile boost</details> |
| [Surformer v1](http://arxiv.org/abs/2508.06566) | Surformer v1: Transformer-Based Surface Classification Using Tactile and Vision Features | Arxiv | 8/2025 |  | <details><summary>Details</summary>Cross-modal attention; 99.4% accuracy, 0.77ms inference; outperforms CNN on efficiency for material recognition</details> |
| [Nonnengieer2025](http://arxiv.org/abs/2506.10787) | In-Hand Object Pose Estimation via Visual-Tactile Fusion | Arxiv | 6/2025 |  | <details><summary>Details</summary>Weighted ICP for RGB-D + fingertip tactile; 7.5mm/16.7° error; 20% better than vision-only under occlusion</details> |
| [Zhang2025a](http://arxiv.org/abs/2502.20367) | The Role of Tactile Sensing for Learning Reach and Grasp | Arxiv | 2/2025 |  | <details><summary>Details</summary>RL evaluation of tactile complexity; simple tactile features improve under visual imperfection</details> |
| [3D-ViTac](http://arxiv.org/abs/2410.24091) | 3D-ViTac: Learning Fine-Grained Manipulation with Visuo-Tactile Sensing | Arxiv | 1/2025 | [Project](https://binghao-huang.github.io/3D-ViTac/) | <details><summary>Details</summary>3mm² dense tactile units; 3D unified representation; outperforms vision-only in fragile + long-horizon tasks</details> |
| [Zhu2025](http://arxiv.org/abs/2507.15062) | Touch in the Wild: Learning Fine-Grained Manipulation with a Portable Visuo-Tactile Gripper | Arxiv | 7/2025 | [Project](https://binghao-huang.github.io/touch_in_the_wild/) | <details><summary>Details</summary>Portable gripper + cross-modal learning; interpretable contact representations; test tube/pipette tasks</details> |
| [Wistreich2025](http://arxiv.org/abs/2509.18830) | DexSkin: High-Coverage Conformable Robotic Skin for Learning Contact-Rich Manipulation | Arxiv | 9/2025 | [Project](https://dex-skin.github.io/) | <details><summary>Details</summary>Capacitive e-skin on full finger surfaces; reorienting + elastic band tasks; cross-instance calibration</details> |
| [Gupta2025](http://arxiv.org/abs/2502.19638) | Sensor-Invariant Tactile Representation | Arxiv | 3/2025 |  | <details><summary>Details</summary>Transformer for zero-shot transfer across optical tactile sensors; diverse simulated designs</details> |
| [OmniVTLA](http://arxiv.org/abs/2508.08706) | OmniVTLA: Vision-Tactile-Language-Action Model with Semantic-Aligned Tactile Sensing | Arxiv | 8/2025 | [Project](https://readerek.github.io/Objtac.github.io) | <details><summary>Details</summary>Dual-path tactile encoder (ViT+SA-ViT); ObjTac dataset; 96.9% gripper/100% dexterous hand success</details> |
| [Tactile-VLA](http://arxiv.org/abs/2507.09160) | Tactile-VLA: Unlocking Vision-Language-Action Model's Physical Knowledge for Tactile Generalization | Arxiv | 7/2025 |  | <details><summary>Details</summary>Hybrid position-force controller + reasoning module; zero-shot contact-rich generalization</details> |
| [PP-Tac](https://peilin-666.github.io/projects/PP-Tac) | PP-Tac: Paper Picking Using Tactile Feedback in Dexterous Robotic Hands | Tech Report | 2025 | [Project](https://peilin-666.github.io/projects/PP-Tac) | <details><summary>Details</summary>Hemispherical R-Tac sensors; diffusion policy for paper/fabric; 87.5% success on deformable objects</details> |
| [Lee2025](http://arxiv.org/abs/2504.15595) | Grasping Deformable Objects via Reinforcement Learning with Cross-Modal Attention to Visuo-Tactile Inputs | Arxiv | 10/2025 |  | <details><summary>Details</summary>DRL with cross-modal attention for visuo-tactile fusion; self-supervised training; outperforms early/late fusion in deformable object grasping</details> |
| [ViTaSCOPE](https://jayjunlee.github.io/vitascope) | ViTaSCOPE: Visuo-tactile Implicit Representation for In-hand Pose and Extrinsic Contact Estimation | Tech Report | 2025 | [Project](https://jayjunlee.github.io/vitascope) | <details><summary>Details</summary>Neural implicit SDF + shear fields; zero-shot Sim2Real; extrinsic contact registration</details> |
| [Guo2025](http://arxiv.org/abs/2506.19303) | Robotic Perception with a Large Tactile-Vision-Language Model for Physical Property Inference | Arxiv | 6/2025 |  | <details><summary>Details</summary>Hierarchical feature alignment; zero-shot generalization on 35 objects; physical property prediction</details> |
| [TranTac](http://arxiv.org/abs/2509.16550) | TranTac: Leveraging Transient Tactile Signals for Contact-Rich Robotic Manipulation | Arxiv | 9/2025 |  | <details><summary>Details</summary>IMU-based transient tactile; 79% insertion success; 88% tactile-only misalignment correction</details> |
| [ViTacFormer](http://arxiv.org/abs/2506.15953) | ViTacFormer: Learning Cross-Modal Representation for Visuo-Tactile Dexterous Manipulation | Arxiv | 6/2025 |  | <details><summary>Details</summary>Cross-attention + autoregressive prediction; 50% higher success; 11-stage long-horizon tasks</details> |
| [exUMI](http://arxiv.org/abs/2509.14688) | exUMI: Extensible Robot Teaching System with Action-aware Task-agnostic Tactile Representation | Arxiv | 9/2025 | [Project](https://silicx.github.io/exUMI) | <details><summary>Details</summary>1M+ tactile frames; TPP pretraining; outperforms traditional tactile IL</details> |
| [Octopi-1.5](https://www.tacniq.ai/tac-02-robotic-finger-dev-kit) | Demonstrating the Octopi-1.5 Visual-Tactile-Language Model | Tech Report | 2025 | [Project](https://www.tacniq.ai/tac-02-robotic-finger-dev-kit) | <details><summary>Details</summary>Multi-part tactile + RAG; handheld TMI interface; live object guessing/teaching</details> |
| [TACT](http://arxiv.org/abs/2506.15146) | TACT: Humanoid Whole-body Contact Manipulation through Deep Imitation Learning with Tactile Modality | IEEE RA-L | 6/2025 |  | <details><summary>Details</summary>Multi-modal ACT policy; upper-body e-skin; balanced walking + manipulation</details>
| [Wu2025b](http://arxiv.org/abs/2409.11047) | TacDiffusion: Force-domain Diffusion Policy for Precise Tactile Manipulation | Arxiv | 3/2025 |  | <details><summary>Details</summary>6D wrench diffusion; 95.7% zero-shot transfer; 9.15% success boost with dynamic filtering</details> |
| [GeoDEx](https://arxiv.org/abs/unknown) | GeoDEx: A Unified Geometric Framework for Tactile Dexterous and Extrinsic Manipulation under Force Uncertainty | Tech Report | 2025 |  | <details><summary>Details</summary>Geometric primitives (plane/cone/ellipsoid); 14x faster than SOCP; stable grasping despite noisy force readings</details> |
| [DEXOP](http://arxiv.org/abs/2509.04441) | DEXOP: A Device for Robotic Transfer of Dexterous Human Manipulation | Arxiv | 9/2025 | [Project](https://dex-op.github.io) | <details><summary>Details</summary>Passive exoskeleton for visuo-tactile data collection; force feedback + pose mirroring; faster/accurate demonstrations than teleop</details> |